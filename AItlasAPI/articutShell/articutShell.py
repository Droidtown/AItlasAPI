#!/usr/bin/env python3
# -*- coding:utf-8 -*-

import os
import re
import sys
import time
from pprint import pprint
from pathlib import Path
import json
from typing import Any, Union

from ArticutAPI import Articut

from articutShell.dynamic_userdefined.dynamic_UD import (
    createUDByGuillemets,
    createCNAMemberUD,
    dict2File,
    dictForger,
    findExactDictionary,
    dictConfirmer,
    _getEntry,
    createAbbrUD
)

G_accountDICT: dict = {}
try:
    with open(Path.cwd()/"account.info", encoding="utf-8") as f:
        G_accountDICT = json.load(f)
    if "url" in G_accountDICT:
        articut = Articut(url=G_accountDICT["url"])
    else:
        articut = Articut(username=G_accountDICT["username"], apikey=G_accountDICT["api_key"])
except:
    print("[articutShell] è«‹å…ˆæ–°å¢ account.info ä¸¦å¡«å…¥ç›¸é—œè³‡è¨Š")
    exit()

if "url" in G_accountDICT:
    # docker ç‰ˆç™»å…¥
    articut = Articut(url=G_accountDICT["url"])
elif "username" and "apikey" in G_accountDICT:
    # ç·šä¸Š ç‰ˆç™»å…¥
    articut = Articut(username=G_accountDICT["username"], apikey=G_accountDICT["apikey"])

G_splitSpecifyPerson_pat = re.compile(r"<ENTITY_person>å‘[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]æ–¹</ENTITY_person>")
G_nameExcept_pat = re.compile(r"å‘[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]æ–¹")
G_extractText_pat = re.compile(r"</?[^>]+>")
G_extracTagAndText_pat = re.compile(r"<([^<]+)>([^<]+)</\1>")
# G_mergeEnglish_pat = re.compile(r"(?<=[a-zA-Z\s])</[a-zA-Z_]+> <[a-zA-Z_]+>(?=[a-zA-Z\s])")
G_mergeByInterpunct_pat = re.compile(
    r"((?<=[a-zA-Z\s])</[a-zA-Z_]+>[Ë‘âˆ˜Â·Ö¼âºâ¦â€§âˆ™â—¦âš«ğ„â¸³â¸°â€¢â—â‹…á›«â¸±êï½¥ï¼ãƒ»Â·]<[a-zA-Z_]+>(?=[a-zA-Z\s]))"
)
G_getTag_pat = re.compile(r"(<[/a-zA-Z_]+>)")
G_getText_pat = re.compile(r"(?<=\>)([^<]+)(?=\<)")
G_pun_pat = re.compile(r"((?<=>)|(?<=^))[^ä¸€-é¾¥a-zA-Z<>]+((?=<)|(?=$))")
G_getQuantifierGroup_pat = re.compile(r"<QUANTIFIER>.</QUANTIFIER><((ENTITY_(?:(oov|noun|nouny|nounHead)))|(FUNC_negation))>(.[æœƒ]?)</((ENTITY_(?:(oov|noun|nouny|nounHead)))|(FUNC_negation))>")
G_getAfterQuantifier_pat = re.compile(r"</((?:ENTITY_(?:oov|noun|nouny|nounHead))|(?:FUNC_negation))>")
G_getColorGroup_pat = re.compile(r"<MODIFIER_color>[^<]+</MODIFIER_color>(<[/a-zA-Z_]+>)(..)(<[/a-zA-Z_]+>)")
G_getAfterColor_pat = re.compile(r"<MODIFIER_color>[^<]+</MODIFIER_color><([/a-zA-Z_]+)>..<[/a-zA-Z_]+>")
G_getLocalityGroup_pat = re.compile(r"<(ENTITY_(?:(oov|noun|nouny|nounHead)))>[^<]+</(ENTITY_(?:(oov|noun|nouny|nounHead)))><RANGE_locality>(?!å…§)[^<]+</RANGE_locality>")
G_getAfterLocality_pat = re.compile(r"</((?:ENTITY_(?:oov|noun|nouny|nounHead))|(?:MODIFIER))><RANGE_locality>[^<]+</RANGE_locality>")
G_getVerbObjectStructure_pat = re.compile(r"(<[^<]+>[æŠ—åè¿”è¦ªä»‡å‹æŒºæ„›æ¯€è³£çŸ¥å‚¾æ»…æ’åŠ©è¯æ´åˆ¶]<[^>]+><[^>]+>[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]+<[^>]+>(<[^>]+>[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]<[^>]+>)*)|(<((?!(LOCATION))[^<]+)>[æŠ—åè¿”è¦ªä»‡å‹æŒºæ„›æ¯€è³£çŸ¥å‚¾æ»…æ’åŠ©è¯æ´åˆ¶][å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]+<((?!(LOCATION))[^<]+)>)(<[^>]+>[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]<[^>]+>)*")
G_getVerbObjectStructurePhrase_pat = re.compile(r"(<[^>]+>[æŠ—åè¿”è¦ªä»‡å‹æŒºæ„›æ¯€è³£çŸ¥å‚¾æ»…æ’åŠ©è¯æ´åˆ¶][å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]+<[^>]+>(<[^>]+>[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]<[^>]+>)*<[^>]+>(åå°‡|è‹±é›„|æˆåŠŸ|æ´¾ç³»|æ´¾|æ°‘æ—|é‡ä»»|è¯ç›Ÿ|æ°£æ°›|æƒ…ç·’|å‹¢åŠ›|è²æµª|ç¾è±¡|æƒ…ç·’|è§€é»|è«–è¿°|è·¯ç·š|ç«‹å ´|æ€æƒ³|æ•™è‚²|æ”¿æ¬Š|åœ˜é«”|è­°æ¡ˆ|æ¡ˆ|æ³•æ¡ˆ|æ¢æ–‡|æ±ºè­°|æ±ºè­°æ–‡|å‹•è­°|åŠ›é‡|å°çµ„|é™£ç·š|è­°å“¡|å­¸è€…|å”æœƒ|ä¸»å¸­|è¨ˆç•«|è¬ è¨€|è«–|è¡Œç‚º|é›†åœ˜|é‹å‹•|è‰²å½©|æš´å‹•|åœ‹å®¶|è·¯å¾‘)<[^>]+>)|(<[^>]+>[æŠ—åè¿”è¦ªä»‡å‹æŒºæ„›æ¯€è³£çŸ¥å‚¾æ»…æ’åŠ©è¯æ´åˆ¶]<[^>]+>(<[^>]+>[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]<[^>]+>)*<[^>]+>[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]+(åå°‡|è‹±é›„|æˆåŠŸ|æ´¾ç³»|æ´¾|æ°‘æ—|é‡ä»»|è¯ç›Ÿ|æ°£æ°›|æƒ…ç·’|å‹¢åŠ›|è²æµª|ç¾è±¡|æƒ…ç·’|è§€é»|è«–è¿°|è·¯ç·š|ç«‹å ´|æ€æƒ³|æ•™è‚²|æ”¿æ¬Š|åœ˜é«”|è­°æ¡ˆ|æ¡ˆ|æ³•æ¡ˆ|æ¢æ–‡|æ±ºè­°|æ±ºè­°æ–‡|å‹•è­°|åŠ›é‡|å°çµ„|é™£ç·š|è­°å“¡|å­¸è€…|å”æœƒ|ä¸»å¸­|è¨ˆç•«|è¬ è¨€|è«–|è¡Œç‚º|é›†åœ˜|é‹å‹•|è‰²å½©|æš´å‹•|åœ‹å®¶|è·¯å¾‘)<[^>]+>)|(<[^>]+>[æŠ—åè¿”è¦ªä»‡å‹æŒºæ„›æ¯€è³£çŸ¥å‚¾æ»…æ’åŠ©è¯æ´åˆ¶][å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]+(åå°‡|è‹±é›„|æˆåŠŸ|æ´¾ç³»|æ´¾|æ°‘æ—|é‡ä»»|è¯ç›Ÿ|æ°£æ°›|æƒ…ç·’|å‹¢åŠ›|è²æµª|ç¾è±¡|æƒ…ç·’|è§€é»|è«–è¿°|è·¯ç·š|ç«‹å ´|æ€æƒ³|æ•™è‚²|æ”¿æ¬Š|åœ˜é«”|è­°æ¡ˆ|æ¡ˆ|æ³•æ¡ˆ|æ¢æ–‡|æ±ºè­°|æ±ºè­°æ–‡|å‹•è­°|åŠ›é‡|å°çµ„|é™£ç·š|è­°å“¡|å­¸è€…|å”æœƒ|ä¸»å¸­|è¨ˆç•«|è¬ è¨€|è«–|è¡Œç‚º|é›†åœ˜|é‹å‹•|è‰²å½©|æš´å‹•|åœ‹å®¶|è·¯å¾‘)<[^>]+>)")
G_getVerbObjectStructurePhraseCountries_pat = re.compile(r"(<[^>]+>[æŠ—åè¿”è¦ªä»‡å‹æŒºæ„›æ¯€è³£çŸ¥å‚¾æ»…æ’åŠ©è¯æ´åˆ¶][å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]+<[^>]+><[^>]+>è»<[^>]+>)|(<[^>]+>[æŠ—åè¿”è¦ªä»‡å‹æŒºæ„›æ¯€è³£çŸ¥å‚¾æ»…æ’åŠ©è¯æ´åˆ¶][å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]*<[^>]+><[^>]+>[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]+è»<[^>]+>)")
G_getCountry_pat = re.compile(r"(<[^>]+>(?!(ç´ç´„)|(ä¸­å…±)|(å·´é»)|(è³½å¡”)|(é¦¬æ–¯å…‹)|(å°è±¡)|(æ¯”è³½))[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]+<[^>]+>)+")
G_getCountryPhrase_pat = re.compile(r"(<[^>]+>[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]+<[^>]+>)+(<[^>]+>[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]<[^>]+>)*<[^>]+>[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]*(é—œä¿‚|å¤§æˆ°|æ–‡åŒ–|äº¤æµ|åˆä½œ|è¡çª)<[^>]+>")
G_getCountryAbbr_pat = re.compile(r"((<[^>]+>[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]<[^>]+>)+(<[^>]+>[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]*[åª’è»ç±æ–¹åºœè£”è‚¡å•†è³‡ç”Ÿä¼åœ‹][^<]*<[^>]+>))|(<(?!UserDefined)[^>]+>[å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]+[åª’è»ç±æ–¹åºœè£”è‚¡å•†è³‡ç”Ÿä¼åœ‹][^<]*<(?!UserDefined)[^>]+>)")
G_getMediaTerm_pat = re.compile(r"(((<[^>]+>[è—ç¶ ç™½]<[^>]+>)+<[^>]+>[è—ç¶ ç™½]*[å§”ç‡Ÿåª’][^<]*<[^>]+>)|((<[^>]+>[è—ç¶ ç™½]<[^>]+>)*<[^>]+>[è—ç¶ ç™½]+[å§”ç‡Ÿåª’][^<]*<[^>]+>))(<[^>]+>äººç‰©<[^>]+>)*")
G_getJobTitle_pat = re.compile(r"(<[^>]+>å‰</[^>]+>)*<KNOWLEDGE_(adminAgency|TWAdminDistrict|TWGov|TWJudicial|TWPresidentialOffice|TWProcuratorate|unitedNationsSystem|organization|location)>[^<]+</KNOWLEDGE_(adminAgency|TWAdminDistrict|TWGov|TWJudicial|TWPresidentialOffice|TWProcuratorate|unitedNationsSystem|organization|location)><[^>]+>é•·<[^>]+>(?!<[^>]+>æ™‚é–“</[^>]+>)")
G_getJobTitlePhrase_pat = re.compile(r"((<[^>]+>å‰</[^>]+>)*<[^>]+>[^<]*[è™•ç§‘é™¢å±€çµ„æœƒå®¤ç½²å®˜éƒ¨é¤¨å¸‚ç¸£æ‰€å·]é•·<[^>]+>)|((<[^>]+>å‰</[^>]+>)*<KNOWLEDGE_chief>[^<]+</KNOWLEDGE_chief>)")
G_getUnit2Num_pat = re.compile(r"<[^<]+>ç¬¬[\dä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+</[^<]+><[^<]+>å±†</[^<]+>")
G_getChemical2Modifier_pat = re.compile(r"<MODIFIER>[^<]+</MODIFIER><KNOWLEDGE_chemical>[^<]+</KNOWLEDGE_chemical>")

# _mergeSpecialVerbAndNoun ç‰¹æ®Šè©åˆæˆ
## æœƒè¦‹
G_hui4Chien4_pat = re.compile(r"<MODAL>æœƒ</MODAL><ACTION_verb>è¦‹</ACTION_verb>")
## å‰æ™¯/å¾Œæ™¯
G_chien2hou4ching3_pat = re.compile(r"<RANGE_locality>[å‰å¾Œ]</RANGE_locality><[^<]+>æ™¯</[^<]+>")


def versions() -> dict[str, str]:
    """
    å›å‚³ç›®å‰çš„ CNA_Articut_VERSION å’Œ CNA_Articut_RELEASEDATE
    """
    versions: dict[str, str] = {
        "CNA_Articut_VERSION": "0.14.1",
        "CNA_Articut_RELEASEDATE": "2025/07/28",
    }
    return versions


def parse(
    inputSTR: str,
    level: str = "lv2",
    chemicalBOOL: bool = True,
    dictDirectorySTR: str = "dict_collection",
    emojiBOOL: bool = True,
    openDataPlaceAccessBOOL: bool = False,
    wikiDataBOOL: bool = False,
    indexWithPOS: bool = False,
    timeRef: Union[Any, None] = None,
    pinyin: str = "BOPOMOFO",
    autoBreakBOOL: bool = True,
    requestID: str = "",
) -> dict:
    """
    åŸ·è¡Œæ–·è©
    """
    startTime: float = time.perf_counter()
    # ç¬¬ä¸€æ¬¡ parse : ç›®çš„æ˜¯æ‹¿å‡ºè¦å»ºç«‹æˆå­—å…¸çš„çŸ¥è­˜ï¼
    firstDICT = articut.parse(
        inputSTR=inputSTR,
        level=level,
        chemicalBOOL=chemicalBOOL,
        emojiBOOL=emojiBOOL,
        openDataPlaceAccessBOOL=openDataPlaceAccessBOOL,
        wikiDataBOOL=wikiDataBOOL,
        indexWithPOS=indexWithPOS,
        timeRef=timeRef,
        pinyin=pinyin,
        autoBreakBOOL=autoBreakBOOL,
        requestID=requestID,
    )
    # å‹•æ…‹å»ºç«‹å­—å…¸
    useDICT: dict[str, list[str]] = {}

    ## articut åŸç”ŸçŸ¥è­˜
    getLIST: list[list[tuple]] = []

    ### æ‹¿ person
    getLIST = getPersonLIST(firstDICT, indexWithPOS=False)
    personLIST: list[str] = []
    if getLIST!=None:
        for get_L in getLIST:
            if len(get_L) != 0 and not _nameExcept(get_L[0][2]):
                personLIST.append(get_L[0][2])
        useDICT.update({"KNOWLEDGE_person": personLIST})

    ### æ‹¿ AddTW
    getLIST = getAddTWLIST(firstDICT, indexWithPOS=False)
    addressTWLIST: list[str] = []
    if getLIST!=None:
        for get_L in getLIST:
            if len(get_L) != 0:
                addressTWLIST.append(get_L[0][2])
        useDICT.update({"KNOWLEDGE_TWAddress": addressTWLIST})

    ### æ‹¿ currency
    getLIST = NER_getMoneyLIST(firstDICT, indexWithPOS=False)
    currencyLIST: list[str] = []
    if getLIST!=None:
        for get_L in getLIST:
            if len(get_L) != 0:
                currencyLIST.append(get_L[0][2])
        useDICT.update({"KNOWLEDGE_currency": currencyLIST})

    ### æ‹¿ LOCATION
    getLIST = getLocationStemLIST(firstDICT, indexWithPOS=False)
    locationLIST: list[str] = []
    if getLIST!=None:
        for get_L in getLIST:
            if len(get_L) != 0:
                locationLIST.append(get_L[0][2])
        useDICT.update({"KNOWLEDGE_location": locationLIST})

    ### æ‹¿ url
    getLIST = NER_getWWWLIST(firstDICT, indexWithPOS=False)
    urlLIST: list[str] = []
    if getLIST!=None:
        for get_L in getLIST:
            if len(get_L) != 0:
                urlLIST.append(get_L[0][2])
        useDICT.update({"KNOWLEDGE_url": urlLIST})

    ## CNA å­—å…¸çŸ¥è­˜
    pieceDICT: dict[str, list[str]] = createUDByGuillemets(inputSTR)
    CNAMemberDICT: dict[str, list[str]] = createCNAMemberUD(inputSTR)
    udDICT: dict[str, list[str]] = dictForger(inputSTR, dictDirectorySTR)

    useDICT.update(pieceDICT)
    useDICT.update(CNAMemberDICT)
    useDICT.update(udDICT)

    # ç¢ºèªå­—å…¸å…§å®¹
    useDICT = dictConfirmer(inputSTR, useDICT)

    # æ–°å¢ç¸®å¯«åŠŸèƒ½
    abbrDICT: dict[str, list[str]] = createAbbrUD(inputSTR, useDICT)
    if len(abbrDICT["KNOWLEDGE_abbr"])!=0:
        useDICT.update(abbrDICT)

    # å°‡å­—å…¸å»ºæˆæš«å­˜æª”
    udFILE = dict2File(useDICT)

    resultDICT = articut.parse(
        inputSTR=inputSTR,
        level=level,
        userDefinedDictFILE=udFILE.name,
        chemicalBOOL=chemicalBOOL,
        emojiBOOL=emojiBOOL,
        openDataPlaceAccessBOOL=openDataPlaceAccessBOOL,
        wikiDataBOOL=wikiDataBOOL,
        indexWithPOS=indexWithPOS,
        timeRef=timeRef,
        pinyin=pinyin,
        autoBreakBOOL=autoBreakBOOL,
        requestID=requestID,
    )

    resultDICT = _splitSpecifyPerson(resultDICT)
    # resultDICT = _mergeEnglish(resultDICT)
    resultDICT = _mergeFragment(resultDICT)
    resultDICT = _tagReplacement(resultDICT, useDICT)
    resultDICT = _mergeJobTitle(resultDICT)
    resultDICT = _createCNATag(resultDICT, directorySTR=dictDirectorySTR)
    resultDICT = _normalizePOS(resultDICT)

    endTime: float = time.perf_counter()
    resultDICT["exec_time"] = endTime-startTime
    return resultDICT


def getAddTWLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    å–å‡ºæ–·è©çµæœä¸­å«æœ‰ (KNOWLEDGE_addTW) æ¨™ç±¤çš„å­—ä¸²ã€‚ è©²å­—ä¸²ç‚ºä¸€å°ç£åœ°å€ã€‚
    """
    return articut.getAddTWLIST(resultDICT, indexWithPOS)


def getChemicalLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    å–å‡ºæ–·è©çµæœä¸­çš„åŒ–å­¸é¡è© (KNOWLEDGE_chemical)ã€‚ æ¯å€‹å¥å­å…§çš„åŒ–å­¸é¡è©ç‚ºä¸€å€‹ listã€‚
    """
    return articut.getChemicalLIST(resultDICT, indexWithPOS)


def getColorLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    å–å‡ºæ–·è©çµæœä¸­å«æœ‰ (MODIFIER_color) æ¨™ç±¤çš„å­—ä¸²ã€‚ è©²å­—ä¸²ç‚ºä¸€é¡è‰²è¡¨è¿°å­—ä¸²ã€‚
    """
    return articut.getColorLIST(resultDICT, indexWithPOS)


def getContentWordLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    å–å‡ºæ–·è©çµæœä¸­çš„å¯¦è© (content word)ã€‚ æ¯å€‹å¥å­å…§çš„å¯¦è©ç‚ºä¸€å€‹ listã€‚
    """
    return articut.getContentWordLIST(resultDICT, indexWithPOS)


def getCurrencyLIST(resultDICT: dict, indexWithPOS: bool = True, greedyBOOL: bool = False) -> list[list]:
    """
    å–å‡ºæ–·è©çµæœä¸­çš„è²¨å¹£é‡‘é¡ (KNOWLEDGE_currency) æ¨™ç±¤çš„å­—ä¸²ã€‚ æ¯å€‹å¥å­å…§çš„ã€Œè²¨å¹£é‡‘é¡ã€ï¼Œå°‡åˆ—ç‚ºä¸€å€‹ listã€‚ è‹¥ greedy = Trueï¼Œå‰‡ä»¥ä¸‹æ ¼å¼æœƒåŠ åˆ°å›å‚³ list
    è²¨å¹£åç¨± + æ•¸å­— (åŒ…å«ã€Œ'ã€èˆ‡ã€Œ,ã€ç¬¦è™Ÿ) æ–°å°å¹£ 100 ç¾é‡‘9.99 æ­å…ƒ 1,999'99
    """
    return articut.getCurrencyLIST(resultDICT, indexWithPOS, greedyBOOL)


def getLocationStemLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    å–å‡ºæ–·è©çµæœä¸­çš„åœ°ç†ä½ç½® (location)ã€‚æ­¤è™•æŒ‡çš„æ˜¯åœ°ç†ä½ç½®æ¨™è¨˜çš„è¡Œæ”¿å€åœ°åè©å½™ï¼Œä¾‹å¦‚ã€Œå°åŒ—ã€ã€ã€Œæ¡ƒåœ’ã€ã€ã€Œå¢¨è¥¿å“¥ã€ã€‚ æ¯å€‹å¥å­å…§çš„åœ°ç†ä½ç½®åˆ—ç‚ºä¸€å€‹ listã€‚
    """
    return articut.getLocationStemLIST(resultDICT, indexWithPOS)


def getNounStemLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    å–å‡ºæ–·è©çµæœä¸­çš„åè© (noun)ã€‚æ­¤è™•æŒ‡çš„æ˜¯ ENTITY_nounã€ENTITY_nounyã€ENTITY_nounHead æˆ– ENTITY_oov æ¨™è¨˜çš„åè©è©å½™ã€‚ æ¯å€‹å¥å­å…§çš„åè©ç‚ºä¸€å€‹ listã€‚
    """
    return articut.getNounStemLIST(resultDICT, indexWithPOS)


def getOpenDataPlaceLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    å–å‡ºæ–·è©çµæœä¸­çš„æ™¯é» (KNOWLEDGE_place) æ¨™ç±¤çš„å­—ä¸²ã€‚æ­¤è™•æŒ‡çš„æ˜¯æ™¯é» (KNOWLEDGE_place)æ¨™è¨˜çš„éè¡Œæ”¿åœ°é»åç¨±è©å½™ï¼Œä¾‹å¦‚ã€Œé¹¿æ¸¯è€è¡—ã€ã€ã€Œå®œè˜­é‹å‹•å…¬åœ’ã€ã€‚ æ¯å€‹å¥å­å…§çš„æ™¯é»ç‚ºä¸€å€‹ listã€‚
    """
    return articut.getOpenDataPlaceLIST(resultDICT, indexWithPOS)


def getPersonLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    å–å‡ºæ–·è©çµæœä¸­çš„äººå (Person) è‹¥ includePronounBOOL ç‚º Trueï¼Œå‰‡é€£ä»£åè© (Pronoun) ä¸€ä½µå›å‚³ï¼›è‹¥ç‚º Falseï¼Œå‰‡åªå›å‚³äººåã€‚ å›å‚³çµæœç‚ºä¸€å€‹ listã€‚
    """
    return articut.getPersonLIST(resultDICT, indexWithPOS)


def getQuestionLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    å–å‡ºæ–·è©çµæœä¸­å«æœ‰ (CLAUSE_Q) æ¨™ç±¤çš„å¥å­ã€‚ æ­¤è™•æŒ‡çš„æ˜¯
    <CLAUSE_AnotAQ>: A-not-A å•å¥
    <CLAUSE_YesNoQ>: æ˜¯éå•å¥
    <CLAUSE_WhoQ">: ã€Œèª°ã€å•å¥
    <CLAUSE_WhatQ>: ã€Œç‰©ã€å•å¥
    <CLAUSE_WhereQ>: ã€Œä½•åœ°ã€å•å¥
    <CLAUSE_WhenQ>: ã€Œä½•æ™‚ã€å•å¥
    <CLAUSE_HowQ>: ã€Œç¨‹åº¦/éç¨‹ã€å•å¥
    <CLAUSE_WhyQ>: ã€ŒåŸå› ã€å•å¥
    æ¯å€‹å¥å­å…§è‹¥æœ‰ <CLAUSE_Q> æ¨™ç±¤ï¼Œæ•´å€‹å¥å­å°‡æœƒå­˜é€² listã€‚
    """
    return articut.getQuestionLIST(resultDICT, indexWithPOS)


def getTimeLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    å–å‡ºæ–·è©çµæœä¸­çš„æ™‚é–“ (time)ã€‚ æ¯å€‹å¥å­å…§çš„ã€Œæ™‚é–“ã€è©åˆ—ç‚ºä¸€å€‹ listã€‚
    """
    return articut.getTimeLIST(resultDICT, indexWithPOS)


def getVerbStemLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    å–å‡ºæ–·è©çµæœä¸­çš„å‹•è© (verb)ã€‚æ­¤è™•æŒ‡çš„æ˜¯ ACTION_verb æ¨™è¨˜çš„å‹•è©è©å½™ã€‚ æ¯å€‹å¥å­å…§çš„å‹•è©ç‚ºä¸€å€‹ listã€‚
    """
    return articut.getVerbStemLIST(resultDICT, indexWithPOS)


def getWikiDataLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    å–å‡ºæ–·è©çµæœä¸­çš„ WikiData æ¨™è¨˜æ–‡å­—ã€‚æ­¤è™•æŒ‡çš„æ˜¯ KNOWLEDGE_wikiData æ¨™è¨˜çš„æ¢ç›®åç¨±ã€‚ æ¯å€‹å¥å­å…§çš„æ¢ç›®åç¨±ç‚ºä¸€å€‹ listã€‚
    """
    return articut.getWikiDataLIST(resultDICT, indexWithPOS)


def NER_getAgeLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„ã€Œæ­²æ•¸ã€å­—ä¸²
    """
    return articut.NER.getAge(resultDICT, indexWithPOS)


def NER_getAngleLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œè§’åº¦ã€çš„å­—ä¸²
    """
    return articut.NER.getAngle(resultDICT, indexWithPOS)


def NER_getAreaLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œé€±é‚Šåœ°å€ã€çš„å­—ä¸²
    """
    return articut.NER.getArea(resultDICT, indexWithPOS)


def NER_getCapacityLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œå®¹é‡ã€çš„å­—ä¸²
    """
    return articut.NER.getCapacity(resultDICT, indexWithPOS)


def NER_getDateLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œæ—¥æœŸã€çš„å­—ä¸²
    """
    return articut.NER.getDate(resultDICT, indexWithPOS)


def NER_getDecimalLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œå°æ•¸ã€çš„å­—ä¸²
    """
    return articut.NER.getDecimal(resultDICT, indexWithPOS)


def NER_getDurationLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œæ™‚é–“å€é–“ã€çš„å­—ä¸²
    """
    return articut.NER.getDuration(resultDICT, indexWithPOS)


def NER_getEmojiLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    å–å‡ºæ–‡æœ¬ä¸­çš„ã€Œemojiã€çš„ç¬¦è™Ÿ
    """
    return articut.NER.getEmoji(resultDICT, indexWithPOS)


def NER_getFoodLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    å–å‡ºæ–‡æœ¬ä¸­çš„é£Ÿç‰©
    """
    return articut.NER.getFood(resultDICT, indexWithPOS)


def NER_getFractionLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œåˆ†æ•¸ã€çš„å­—ä¸²
    """
    return articut.NER.getFraction(resultDICT, indexWithPOS)


def NER_getFrequencyLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œé »ç‡ã€çš„å­—ä¸²
    """
    return articut.NER.getFrequency(resultDICT, indexWithPOS)


def NER_getIntegerLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œæ•´æ•¸ã€çš„å­—ä¸²
    """
    return articut.NER.getInteger(resultDICT, indexWithPOS)


def NER_getLengthLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œé•·åº¦ã€çš„å­—ä¸²
    """
    return articut.NER.getLength(resultDICT, indexWithPOS)


def NER_getLocationLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œé‡é‡ã€çš„å­—ä¸²ã€‚
    æ­¤åŠŸèƒ½å’Œ ArticutAPI ä¸­çš„ getLoctionStemLIST() ç­‰æ•ˆã€‚
    """
    return articut.NER.getLocation(resultDICT, indexWithPOS)


def NER_getMeasureLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­ç‚ºã€Œæ¸¬é‡å€¼ã€çš„å­—ä¸²
    """
    return articut.NER.getMeasure(resultDICT, indexWithPOS)


def NER_getMoneyLIST(resultDICT: dict, greedyBOOL: bool = False, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œé‡‘é¡ã€çš„å­—ä¸²
    æ­¤åŠŸèƒ½å’Œ ArticutAPI ä¸­çš„ getCurrencyLIST() ç­‰æ•ˆã€‚
    """
    return articut.NER.getMoney(resultDICT, greedyBOOL, indexWithPOS)


def NER_getOrdinalLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œåºæ•¸ã€çš„å­—ä¸²
    """
    return articut.NER.getOrdinal(resultDICT, indexWithPOS)


def NER_getPercentLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œç™¾åˆ†æ¯”/åƒåˆ†æ¯”/è¬åˆ†æ¯”ã€çš„å­—ä¸²
    """
    return articut.NER.getPercent(resultDICT, indexWithPOS)


def NER_getPersonLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œäººåã€çš„å­—ä¸²
    æ­¤åŠŸèƒ½å’Œ ArticutAPI ä¸­çš„ getPersonLIST() ç­‰æ•ˆã€‚
    å–å‡ºæ–·è©çµæœä¸­çš„äººå (Person)
    è‹¥ includePronounBOOL ç‚º Trueï¼Œå‰‡é€£ä»£åè© (Pronoun) ä¸€ä½µå›å‚³ï¼›è‹¥ç‚º Falseï¼Œå‰‡åªå›å‚³äººåã€‚
    """
    return articut.NER.getPerson(resultDICT, indexWithPOS)


def NER_getRateLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œæ¯”ä¾‹ã€çš„å­—ä¸²
    """
    return articut.NER.getRate(resultDICT, indexWithPOS)


def NER_getSpeedLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œé€Ÿåº¦ã€çš„å­—ä¸²
    """
    return articut.NER.getSpeed(resultDICT, indexWithPOS)


def NER_getTemperatureLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œæº«åº¦ã€çš„å­—ä¸²
    """
    return articut.NER.getSpeed(resultDICT, indexWithPOS)


def NER_getTimeLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œæ™‚é–“ã€çš„å­—ä¸²
    """
    return articut.NER.getSpeed(resultDICT, indexWithPOS)


def NER_getWeightLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€Œé‡é‡ã€çš„å­—ä¸²
    """
    return articut.NER.getWeight(resultDICT, indexWithPOS)


def NER_getWWWLIST(resultDICT: dict, indexWithPOS: bool = True) -> list[list]:
    """
    ä¾ MSRA (å¾®è»Ÿäºæ´²ç ”ç©¶é™¢, Microsoft Research Lab Asia) NER æ¨™æº–å–å‡ºæ–‡æœ¬ä¸­çš„æè¿°ã€ŒURL é€£çµã€çš„å­—ä¸²
    """
    return articut.NER.getWWW(resultDICT, indexWithPOS)


def lawsToolkit_getCrimeLIST(resultDICT: dict) -> list[list]:
    """
    å–å¾—ç½ªåã€‚
    """
    return articut.LawsToolkit.getCrime(resultDICT)


def lawsToolkit_getPenaltyLIST(resultDICT: dict) -> list[list]:
    """
    å–å¾—åˆ‘è²¬ã€‚
    """
    return articut.LawsToolkit.getPenalty(resultDICT)


def lawsToolkit_getLawArticleLIST(resultDICT: dict) -> list[list]:
    """
    å–å¾—æ³•æ¢ç·¨è™Ÿã€‚
    """
    return articut.LawsToolkit.getLawArticle(resultDICT)


def localRE_getAddressCountyLIST(
    resultDICT: dict, indexWithPOS: bool = True
) -> list[list]:
    """
    å–å‡º Articut æ–·è©çµæœä¸­æ¨™è¨˜å–å‡ºæ˜¯å“ªå€‹ã€Œç¸£ã€ã€‚
    """
    return articut.localRE.getAddressCounty(resultDICT, indexWithPOS)


def localRE_getAddressCityLIST(
    resultDICT: dict, indexWithPOS: bool = True
) -> list[list]:
    """
    å–å‡º Articut æ–·è©çµæœä¸­æ¨™è¨˜å–å‡ºæ˜¯å“ªå€‹ã€Œå¸‚ã€ã€‚
    """
    return articut.localRE.getAddressCity(resultDICT, indexWithPOS)


def localRE_getAddressDistrictLIST(
    resultDICT: dict, indexWithPOS: bool = True
) -> list[list]:
    """
    å–å‡º Articut æ–·è©çµæœä¸­æ¨™è¨˜å–å‡ºæ˜¯å“ªå€‹ã€Œå€ã€ã€‚
    """
    return articut.localRE.getAddressDistrict(resultDICT, indexWithPOS)


def localRE_getAddressTownshipLIST(
    resultDICT: dict, indexWithPOS: bool = True
) -> list[list]:
    """
    å–å‡º Articut æ–·è©çµæœä¸­æ¨™è¨˜å–å‡ºæ˜¯å“ªå€‹ã€Œé„‰ã€æˆ–ã€Œé‡Œã€ã€‚
    """
    return articut.localRE.getAddressTownship(resultDICT, indexWithPOS)


def localRE_getAddressTownLIST(
    resultDICT: dict, indexWithPOS: bool = True
) -> list[list]:
    """
    å–å‡º Articut æ–·è©çµæœä¸­æ¨™è¨˜å–å‡ºæ˜¯å“ªå€‹ã€Œé®ã€ã€‚
    """
    return articut.localRE.getAddressTown(resultDICT, indexWithPOS)


def localRE_getAddressVillageLIST(
    resultDICT: dict, indexWithPOS: bool = True
) -> list[list]:
    """
    å–å‡º Articut æ–·è©çµæœä¸­æ¨™è¨˜å–å‡ºæ˜¯å“ªå€‹ã€Œæ‘ã€ã€‚
    """
    return articut.localRE.getAddressVillage(resultDICT, indexWithPOS)


def localRE_getAddressNeighborhoodLIST(
    resultDICT: dict, indexWithPOS: bool = True
) -> list[list]:
    """
    å–å‡º Articut æ–·è©çµæœä¸­æ¨™è¨˜å–å‡ºæ˜¯å“ªå€‹ã€Œé„°ã€ã€‚
    """
    return articut.localRE.getAddressNeighborhood(resultDICT, indexWithPOS)


def localRE_getAddressRoadLIST(
    resultDICT: dict, indexWithPOS: bool = True
) -> list[list]:
    """
    å–å‡º Articut æ–·è©çµæœä¸­æ¨™è¨˜å–å‡ºæ˜¯å“ªæ¢ã€Œè·¯ã€ã€‚
    """
    return articut.localRE.getAddressRoad(resultDICT, indexWithPOS)


def localRE_getAddressSectionLIST(
    resultDICT: dict, indexWithPOS: bool = True
) -> list[list]:
    """
    å–å‡º Articut æ–·è©çµæœä¸­æ¨™è¨˜å–å‡ºæ˜¯å“ªä¸€ã€Œæ®µã€ã€‚
    """
    return articut.localRE.getAddressSection(resultDICT, indexWithPOS)


def localRE_getAddressAlleyLIST(
    resultDICT: dict, indexWithPOS: bool = True
) -> list[list]:
    """
    å–å‡º Articut æ–·è©çµæœä¸­æ¨™è¨˜å–å‡ºæ˜¯å“ªä¸€ã€Œå··ã€æˆ–ã€Œå¼„ã€ã€‚
    """
    return articut.localRE.getAddressAlley(resultDICT, indexWithPOS)


def localRE_getAddressNumberLIST(
    resultDICT: dict, indexWithPOS: bool = True
) -> list[list]:
    """
    å–å‡º Articut æ–·è©çµæœä¸­æ¨™è¨˜å–å‡ºæ˜¯å¹¾ã€Œè™Ÿã€ã€‚
    """
    return articut.localRE.getAddressNumber(resultDICT, indexWithPOS)


def localRE_getAddressFloorLIST(
    resultDICT: dict, indexWithPOS: bool = True
) -> list[list]:
    """
    å–å‡º Articut æ–·è©çµæœä¸­æ¨™è¨˜å–å‡ºæ˜¯å¹¾ã€Œæ¨“ã€ã€‚
    """
    return articut.localRE.getAddressFloor(resultDICT, indexWithPOS)


def localRE_getAddressRoomLIST(
    resultDICT: dict, indexWithPOS: bool = True
) -> list[list]:
    """
    å–å‡º Articut æ–·è©çµæœä¸­æ¨™è¨˜å–å‡ºã€Œå®¤ã€çš„ç·¨è™Ÿã€‚
    """
    return articut.localRE.getAddressRoom(resultDICT, indexWithPOS)


def _tagReplacement(resultDICT: dict, myDICT: dict[str, list[str]]) -> dict:
    """
    å°‡ userdefinedçš„è©èª tag æ›æˆ <KNOWLEDGE_xxx> çš„æ ¼å¼ã€‚
    """
    # æ›´æ”¹ result_obj
    ## ä¸€èˆ¬å­—å…¸
    for sentencesLIST in resultDICT["result_obj"]:
        for wordDICT in sentencesLIST:
            if wordDICT["pos"] == "UserDefined":
                wordDICT["pos"] = findExactDictionary(wordDICT["text"], myDICT)

    # æ›´æ”¹ result_pos
    resultDICT["result_pos"] = _getResultPosByResultObj(resultDICT["result_obj"])

    return resultDICT


def _getResultObjByResultPos(resultPos: list[str]) -> list[list[dict[str, str]]]:
    """
    ç¶“ç”± resultPos å¾—åˆ°å°æ‡‰çš„ resultObj
    """
    newResultObj: list[list[dict]] = []
    for sentence_S in resultPos:
        sentenceLIST: list[dict] = []
        itemLIST: list[tuple[str, str]] = [
            x.groups() for x in G_extracTagAndText_pat.finditer(sentence_S)
        ]

        if len(itemLIST) == 0:
            itemDICT: dict[str, str] = {}
            itemDICT["pos"] = "PUNCTUATION"
            itemDICT["text"] = sentence_S
            sentenceLIST.append(itemDICT)
            newResultObj.append(sentenceLIST)
            continue

        for item_T in itemLIST:
            itemDICT: dict[str, str] = {}
            itemDICT["pos"] = item_T[0]
            itemDICT["text"] = item_T[1]
            sentenceLIST.append(itemDICT)

        newResultObj.append(sentenceLIST)

    return newResultObj


def _getResultSegmentationByResultPos(resultPos: list[str]) -> list[str]:
    """
    ç¶“ç”± resultPos å¾—åˆ°å°æ‡‰çš„ resultSegmentation
    """
    newResultSegmentation: list[str] = []
    for sentence_S in resultPos:
        nameLIST: list[str] = re.findall(r"<[a-zA-Z]", sentence_S)
        if len(nameLIST) == 0:
            newResultSegmentation.append(sentence_S)
            continue

        newSentenceSTR: str = re.sub(G_extractText_pat, "/", sentence_S)
        newSentenceSTR = newSentenceSTR.replace("//", "/")
        newSentenceSTR = newSentenceSTR.strip("/")
        newResultSegmentation.append(newSentenceSTR)

    return newResultSegmentation


def _getResultPosByResultObj(resultObj: list[list[dict[str, str]]]) -> list[str]:
    """
    ç¶“ç”± resultObj å¾—åˆ°å°æ‡‰çš„ resultPos
    """
    newResultPosLIST: list[str] = []
    for sentences_L in resultObj:
        sentenceSTR: str = ""
        for word_D in sentences_L:
            if word_D["pos"] == "PUNCTUATION":
                sentenceSTR += word_D["text"]
            else:
                sentenceSTR += "<"
                sentenceSTR += word_D["pos"]
                sentenceSTR += ">"
                sentenceSTR += word_D["text"]
                sentenceSTR += "<"
                endTag: str = word_D["pos"][:0] + "/" + word_D["pos"][0:]
                sentenceSTR += endTag
                sentenceSTR += ">"

        newResultPosLIST.append(sentenceSTR)

    return newResultPosLIST


def _createCNATag(resultDICT: dict, directorySTR: str) -> dict:
    """
    æ–°å¢ä¸€å€‹ Key ç‚º CNATAGï¼Œå°‡æ‰€æœ‰è©²æ–‡æœ¬ä¸­ä½¿ç”¨åˆ°çš„ <KNOWLEDGE_xxx> ç´€éŒ„æ–¼æ­¤ã€‚
    <KNOWLEDGE_xxx> æœƒç¶“éæª¢æŸ¥ï¼Œä¸€å¾‹è¼¸å‡ºè©²ç­†å…§å®¹çš„ keyã€‚
    """
    CNATag: dict[str, dict[str, list[str]]] = {}
    KNOWLEDGE_pat = r"(KNOWLEDGE_.*|.*_dict)"

    for sentenceLIST in resultDICT["result_obj"]:
        for wordDICT in sentenceLIST:
            posSTR = wordDICT["pos"]
            textSTR = wordDICT["text"]

            match: Union[(re.Match[str]), (None)] = re.search(KNOWLEDGE_pat, posSTR)

            if match:
                entryDICT: dict[str, list[str]] = _getEntry(posSTR, textSTR, directorySTR)

                if posSTR not in CNATag:
                    CNATag[posSTR] = {}

                CNATag[posSTR].update(entryDICT)

    resultDICT["CNA_tag"] = CNATag
    return resultDICT

def _normalizePOS(resultDICT: dict) -> dict:
    """
    æ­£è¦åŒ– LOCATION
    """
    for sentencesLIST in resultDICT["result_obj"]:
        for wordDICT in sentencesLIST:
            if wordDICT["pos"] == "LOCATION":
                wordDICT["pos"] = "KNOWLEDGE_location"

    # æ›´æ”¹ result_pos
    resultDICT["result_pos"] = _getResultPosByResultObj(resultDICT["result_obj"])

    return resultDICT

def _nameExcept(nameSTR: str) -> bool:
    """
    æ˜¯å¦è©²æ’é™¤æ­¤äººåä½œç‚º personã€‚
    å¦‚ï¼šå‘ç¾æ–¹ -> true
    """
    nameLIST: list[str] = [x.groups() for x in G_nameExcept_pat.finditer(nameSTR)]
    if len(nameLIST) != 0:
        return True
    return False

def _splitSpecifyPerson(resultDICT: dict) -> dict:
    """
    æª¢æŸ¥æ¨™è¨˜ç‚º Person çš„ entityï¼Œè‹¥ç¬¦åˆç‰¹å®šæ ¼å¼ï¼Œæ‹†é–‹ã€‚
    å¦‚ï¼š<ENTITY_person>å‘ç¾æ–¹</ENTITY_person> -> <FUNC_inner>å‘</FUNC_inner><ENTITY_noun>ç¾æ–¹</<ENTITY_noun>>
    """
    newResultPosLIST: list[str] = _addSymbolToPunc(resultDICT["result_pos"])
    joinSTR: str = "".join(newResultPosLIST)

    ### æ‰¾åˆ°çµæ§‹
    personLIST = [x.group() for x in G_splitSpecifyPerson_pat.finditer(joinSTR)]

    if len(personLIST)==0:
        return resultDICT

    ### åˆæˆæ–°çµæ§‹
    newPersonLIST = []
    for structure in personLIST:
        struLIST: list[str] = [x.group() for x in G_getText_pat.finditer(structure)]
        newPersonLIST.append("<FUNC_inner>å‘</FUNC_inner><ENTITY_noun>" + str(struLIST[0][1:]) + "</ENTITY_noun>")

    ### ä»£æ›¿åŸçµæ§‹
    for i in range(len(personLIST)):
        joinSTR = re.sub(personLIST[i], newPersonLIST[i], joinSTR)

    # æ ¹æ“šæ–°çš„ result_pos ä¿®æ”¹ result_segmentation & result_obj
    resultPosLIST: list[str] = _splitResultPosSTR(joinSTR)
    resultDICT["result_pos"] = resultPosLIST
    resultDICT["result_obj"] = _getResultObjByResultPos(resultPosLIST)
    resultDICT["result_segmentation"] = _getResultSegmentationByResultPos(resultPosLIST)

    return resultDICT


def _mergeEnglish(resultDICT: dict) -> dict:
    print(resultDICT["result_pos"])
    joinSTR: str = "".join(resultDICT["result_pos"])
    print(joinSTR)
    # mergeEngSTR: str = G_mergeEnglish_pat.sub(" ", joinSTR)
    # newResultPOSLIST: list[str] = re.split(r"(?<=>).(?=<)", mergeEngSTR)
    # print(newResultPOSLIST)

    # resultDICT["result_pos"] = newResultPOSLIST
    # resultDICT["result_obj"] = _getResultObjByResultPos(newResultPOSLIST)
    # resultDICT["result_segmentation"] = _getResultSegmentationByResultPos(
    #     newResultPOSLIST
    # )

    return resultDICT

def _mergeFragment(resultDICT: dict) -> dict:
    """
    åˆä½µ MODIFIER_colorã€quantifier(å…¨ã€åŠã€éƒ½)ã€RANGE_locality
    """
    resultDICT = _mergeByInterpunct(resultDICT)
    resultDICT = _mergeQuantifier(resultDICT)
    resultDICT = _mergeColor(resultDICT)
    resultDICT = _mergeSpecialVerbAndNoun(resultDICT)
    resultDICT = _mergeVerbObjectStructure(resultDICT)
    resultDICT = _mergeCountry(resultDICT)
    resultDICT = _mergeCountryAbbr(resultDICT)
    resultDICT = _mergeMediaTerm(resultDICT)
    resultDICT = _mergeUnit2Num(resultDICT)
    resultDICT = _mergeChemical2Modifier(resultDICT)
    resultDICT = _mergeLocality(resultDICT)

    return resultDICT

def _mergeByInterpunct(resultDICT: dict) -> dict:
    """
    å°‡ articut æ–·è©çµæœå¾Œè™•ç†ã€‚è‹¥é‡åˆ°åˆ†éš”è™Ÿï¼Œå‰‡å°‡å‰å¾Œå…ƒç´ åˆä½µã€‚
    """
    newResultPosLIST: list[str] = _addSymbolToPunc(resultDICT["result_pos"])
    joinSTR: str = "".join(newResultPosLIST)

    ## æ‰¾åˆ†éš”è™Ÿçµæ§‹ã€‚
    patternLIST: list[tuple[str, str]] = [
        x.groups() for x in G_mergeByInterpunct_pat.finditer(joinSTR)
    ]
    replacePatternLIST: list[str] = []
    if len(patternLIST) == 0:
        return resultDICT

    ## å°åˆ†éš”è™Ÿçµæ§‹åšç²¾ç°¡ï¼Œå–å‡ºä¹‹å¾Œè¦ç”¨çš„åˆ†éš”è™Ÿã€‚
    for pattern_T in patternLIST:
        interpunct_S: str = G_getTag_pat.sub("", pattern_T[0])
        replacePatternLIST.append(interpunct_S)

    ## å°æ¯å€‹åˆ†éš”è™Ÿçµæ§‹åšå–ä»£ï¼Œå–ä»£æˆåˆ†éš”è™Ÿã€‚
    for i in range(len(patternLIST)):
        joinSTR = re.sub(patternLIST[i][0], replacePatternLIST[i], joinSTR)

    resultPosLIST: list[str] = _splitResultPosSTR(joinSTR)

    # èƒå–å‡ºæ–°çš„ result_objã€result_pos_result_segmentation
    resultDICT["result_pos"] = resultPosLIST
    resultDICT["result_obj"] = _getResultObjByResultPos(resultPosLIST)
    resultDICT["result_segmentation"] = _getResultSegmentationByResultPos(resultPosLIST)

    return resultDICT

def _mergeQuantifier(resultDICT: dict) -> dict:
    """
    åˆä½µ QUANTIFIER ä¸­å…¨ã€åŠã€éƒ½èˆ‡å…¶å¾Œæ–¹åˆé©çš„è©(1å€‹éŸ³ç¯€)
    """
    # è™•ç†æ¨™ç±¤

    ## join result_pos
    newResultPosLIST: list[str] = _addSymbolToPunc(resultDICT["result_pos"])
    joinSTR: str = "".join(newResultPosLIST)

    ## æ‰¾çµæ§‹ã€‚
    patternLIST: list[str] = [x.group() for x in G_getQuantifierGroup_pat.finditer(joinSTR)]
    if len(patternLIST)==0:
        return resultDICT

    ## ç²¾ç°¡çµæ§‹ï¼Œå–å‡ºä¹‹å¾Œè¦ç”¨çš„ç´ æã€‚
    nameLIST: list[str] = []
    tagLIST: list[list[tuple[str, str]]] = []
    for pattern_S in patternLIST:
        tmpLIST: list[tuple[str, str]] = [x.groups() for x in G_getText_pat.finditer(pattern_S)]
        nameSTR: str = ""
        for name_T in tmpLIST:
            nameSTR= nameSTR + name_T[0]

        nameLIST.append(nameSTR)
        tagLIST.append([x.groups() for x in G_getAfterQuantifier_pat.finditer(pattern_S)])

    ## åˆæˆæ–°çµæ§‹
    replaceLIST: list[str] = []
    for i in range(len(tagLIST)):
        replaceLIST.append("<" + tagLIST[i][0][0] + ">" + nameLIST[i] + "</" + tagLIST[i][0][0] + ">")

    ## ä»¥æ–°çµæ§‹å–ä»£åŸçµæ§‹
    for i in range(len(patternLIST)):
        joinSTR = re.sub(patternLIST[i], replaceLIST[i], joinSTR)

    resultPosLIST: list[str] = _splitResultPosSTR(joinSTR)

    # æ›´æ–° result_posã€ result_obj å’Œ result_segmentation
    resultDICT["result_pos"] = resultPosLIST
    resultDICT["result_obj"] = _getResultObjByResultPos(resultPosLIST)
    resultDICT["result_segmentation"] = _getResultSegmentationByResultPos(resultPosLIST)

    return resultDICT

def _mergeColor(resultDICT: dict) -> dict:
    """
    åˆä½µ MODIFIER_color èˆ‡å…¶å¾Œæ–¹åˆé©çš„è©(2å€‹éŸ³ç¯€)
    """
    # è™•ç† color
    ## join result_pos
    newPosLIST: list[str] = _addSymbolToPunc(resultDICT["result_pos"])
    joinSTR: str = "".join(newPosLIST)

    ## æ‰¾çµæ§‹ã€‚
    patternLIST: list[str] = [x.group() for x in G_getColorGroup_pat.finditer(joinSTR)]
    if len(patternLIST)==0:
        return resultDICT

    ## ç²¾ç°¡çµæ§‹ï¼Œå–å‡ºä¹‹å¾Œè¦ç”¨çš„ç´ æã€‚
    nameLIST: list[str] = []
    tagLIST: list[list[tuple[str, str]]] = []
    for pattern_S in patternLIST:
        tmpLIST: list[tuple[str, str]] = [x.groups() for x in G_getText_pat.finditer(pattern_S)]
        nameSTR: str = ""
        for name_T in tmpLIST:
            nameSTR= nameSTR + name_T[0]

        nameLIST.append(nameSTR)
        tagLIST.append([x.groups() for x in G_getAfterColor_pat.finditer(pattern_S)])

    ## åˆæˆæ–°çµæ§‹
    replaceLIST: list[str] = []
    for i in range(len(tagLIST)):
        replaceLIST.append("<" + tagLIST[i][0][0] + ">" + nameLIST[i] + "</" + tagLIST[i][0][0] + ">")

    ## ä»¥æ–°çµæ§‹å–ä»£åŸçµæ§‹
    for i in range(len(patternLIST)):
        joinSTR = re.sub(patternLIST[i], replaceLIST[i], joinSTR)

    resultPosLIST: list[str] = _splitResultPosSTR(joinSTR)

    # æ›´æ–° result_posã€ result_obj å’Œ result_segmentation
    resultDICT["result_pos"] = resultPosLIST
    resultDICT["result_obj"] = _getResultObjByResultPos(resultPosLIST)
    resultDICT["result_segmentation"] = _getResultSegmentationByResultPos(resultPosLIST)

    return resultDICT

def _mergeSpecialVerbAndNoun(resultDICT: dict) -> dict:
    """
    åˆæˆä¸é©åˆåœ¨ Articut åº•å±¤è™•ç†çš„è©
    e.g.ï¼šæœƒè¦‹(ä¸­å¤æ¼¢èª)
    """
    # éæ­·æª¢æŸ¥ result_pos
    newPosLIST: list[str] = _addSymbolToPunc(resultDICT["result_pos"])
    joinSTR: str = "".join(newPosLIST)

    ## è™•ç†å‹•è©
    ### æ‰¾åˆ°çµæ§‹
    hui4Chien4LIST: list[str] = [x.group() for x in G_hui4Chien4_pat.finditer(joinSTR)]

    if len(hui4Chien4LIST)!=0:
        ### åˆæˆæ–°çµæ§‹
        newHui4Chien4LIST = []
        for structure in hui4Chien4LIST:
            struLIST: list[str] = [x.group() for x in G_getText_pat.finditer(structure)]
            struSTR: str = "".join(struLIST)
            newHui4Chien4LIST.append("<ACT_verb>" + struSTR + "</ACT_verb>")

        ### ä»£æ›¿åŸçµæ§‹
        for i in range(len(hui4Chien4LIST)):
            joinSTR = re.sub(hui4Chien4LIST[i], newHui4Chien4LIST[i], joinSTR)

    ## è™•ç†åè©
    ### æ‰¾åˆ°çµæ§‹
    chien2hou4ching3LIST: list[str] = [x.group() for x in G_chien2hou4ching3_pat.finditer(joinSTR)]

    if len(chien2hou4ching3LIST)!=0:
        ### åˆæˆæ–°çµæ§‹
        newChien2hou4ching3LIST = []
        for structure in chien2hou4ching3LIST:
            struLIST: list[str] = [x.group() for x in G_getText_pat.finditer(structure)]
            struSTR: str = "".join(struLIST)
            newChien2hou4ching3LIST.append("<ENTY_noun>" + struSTR + "</ENTY_noun>")

        ### ä»£æ›¿åŸçµæ§‹
        for i in range(len(chien2hou4ching3LIST)):
            joinSTR = re.sub(chien2hou4ching3LIST[i], newChien2hou4ching3LIST[i], joinSTR)

    # æ ¹æ“šæ–°çš„ result_pos ä¿®æ”¹ result_segmentation & result_obj
    resultPosLIST: list[str] = _splitResultPosSTR(joinSTR)
    resultDICT["result_pos"] = resultPosLIST
    resultDICT["result_obj"] = _getResultObjByResultPos(resultPosLIST)
    resultDICT["result_segmentation"] = _getResultSegmentationByResultPos(resultPosLIST)

    return resultDICT

def _mergeLocality(resultDICT: dict) -> dict:
    """
    åˆä½µåˆé©çš„è©èˆ‡å…¶å¾Œæ–¹çš„ RANGE_locality 
    """
    # è™•ç† RANGE_locality

    ## join result_pos
    newPosLIST: list[str] = _addSymbolToPunc(resultDICT["result_pos"])
    joinSTR: str = "".join(newPosLIST)

    ## æ‰¾çµæ§‹ã€‚
    patternLIST: list[str] = [x.group() for x in G_getLocalityGroup_pat.finditer(joinSTR)]
    if len(patternLIST)==0:
        return resultDICT

    ## ç²¾ç°¡çµæ§‹ï¼Œå–å‡ºä¹‹å¾Œè¦ç”¨çš„ç´ æã€‚
    nameLIST: list[str] = []
    tagLIST: list[list[tuple[str, str]]] = []
    for pattern_S in patternLIST:
        tmpLIST: list[tuple[str, str]] = [x.groups() for x in G_getText_pat.finditer(pattern_S)]
        nameSTR: str = ""
        for name_T in tmpLIST:
            nameSTR= nameSTR + name_T[0]

        nameLIST.append(nameSTR)
        tagLIST.append([x.groups() for x in G_getAfterLocality_pat.finditer(pattern_S)])

    ## åˆæˆæ–°çµæ§‹
    replaceLIST: list[str] = []
    for i in range(len(tagLIST)):
        replaceLIST.append("<" + tagLIST[i][0][0] + ">" + nameLIST[i] + "</" + tagLIST[i][0][0] + ">")

    ## ä»¥æ–°çµæ§‹å–ä»£åŸçµæ§‹
    for i in range(len(patternLIST)):
        joinSTR = re.sub(patternLIST[i], replaceLIST[i], joinSTR)

    resultPosLIST: list[str] = _splitResultPosSTR(joinSTR)

    # æ›´æ–° result_posã€ result_obj å’Œ result_segmentation
    resultDICT["result_pos"] = resultPosLIST
    resultDICT["result_obj"] = _getResultObjByResultPos(resultPosLIST)
    resultDICT["result_segmentation"] = _getResultSegmentationByResultPos(resultPosLIST)

    return resultDICT

def _mergeVerbObjectStructure(resultDICT: dict) -> dict:
    """
    VerbObjectStructureï¼šå‹•è³“çµæ§‹ï¼Œæ¨™è¨˜ POS ç‚º ACT_intlRelation æˆ– é€²ä¸€æ­¥åˆæˆ ENTY_intlRelation
    å¦‚ï¼š[æŠ—åè¿”è¦ªä»‡å‹æŒºæ„›æ¯€è³£çŸ¥å‚¾æ»…æ’åŠ©è¯æ´åˆ¶][å°è‡ºè‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­å…±æ¾³è¶Šæ—¥æœæ˜Ÿç´é¦¬è²ç¾©è¯ç“œå¸›çƒè·ä»¥é˜¿ç·¬æ³¢å¸ƒè’²å–€è±¡åŸƒæ¸¯å„è¡£ç”˜åŠ è‚¯è³´æ‘©è«ç´å¥ˆç›§è–å¡ç…ç´¢å²å¦çªè¾›äºå·´å­Ÿæ±¶æŸ¬è³½å–¬ä¼Šç´„å“ˆé»è’™å°¼æ²™æ–¯æ•˜å¡”åœŸå¥§æ¯”ä¿å…‹èŠ¬åŒˆæ‹‰å°è‘¡ç¾…è¥¿å“¥è–©å®ç‰™å¢¨ç»è“‹ç§˜å§”æ–å‰è«¾å]
    """
    # éæ­·æª¢æŸ¥ result_pos
    newPosLIST: list[str] = _addSymbolToPunc(resultDICT["result_pos"])
    joinSTR: str = "".join(newPosLIST)

    ## è¢«æ–·æˆå…©åŠçš„çµæ§‹ï¼Œåˆèµ·ä¾†ã€‚å¦‚ï¼šæŠ—/ä¸­ -> æŠ—ä¸­ã€‚ä¸¦å’Œæ‰€æœ‰å±¬æ–¼ä¸Šè¿°å‹•è³“çµæ§‹çš„ç¯„ä¾‹ä¸€åŒä¿®æ”¹ POS ç‚º ACT_intlRelationã€‚
    ### æ‰¾åˆ°çµæ§‹
    verbPLIST: list[str] = [x.group() for x in G_getVerbObjectStructure_pat.finditer(joinSTR)]

    if len(verbPLIST)!=0:
        ### åˆæˆæ–°çµæ§‹
        newVerbPLIST: list[str] = []
        for structure in verbPLIST:
            struLIST: list[str] = [x.group() for x in G_getText_pat.finditer(structure)]
            struSTR: str = "".join(struLIST)
            newVerbPLIST.append("<ACT_intlRelation>" + struSTR + "</ACT_intlRelation>")

        ### ä»£æ›¿åŸçµæ§‹
        for i in range(len(verbPLIST)):
            joinSTR = re.sub(verbPLIST[i], newVerbPLIST[i], joinSTR)

    ## é¡å¤–è™•ç†ç‰¹æ®Šåè©(å–®åœ‹)ï¼Œåˆèµ·ä¾†ã€‚å¦‚ï¼šæŠ—x/å…ƒç´ ã€æŠ—/xå…ƒç´  åˆèµ·ä¾†ï¼ŒPOS è¨­æˆ ENTITY_intlRelation
    ### æ‰¾åˆ°çµæ§‹
    verbPLIST = [x.group() for x in G_getVerbObjectStructurePhrase_pat.finditer(joinSTR)]

    ### åˆæˆæ–°çµæ§‹
    newVerbPLIST = []
    for structure in verbPLIST:
        struLIST: list[str] = [x.group() for x in G_getText_pat.finditer(structure)]
        struSTR: str = "".join(struLIST)
        newVerbPLIST.append("<ENTY_intlRelation>" + struSTR + "</ENTY_intlRelation>")

    ### ä»£æ›¿åŸçµæ§‹
    for i in range(len(verbPLIST)):
        joinSTR = re.sub(verbPLIST[i], newVerbPLIST[i], joinSTR)

    ## é¡å¤–è™•ç†ç‰¹æ®Šåè©(å¤šåœ‹)ï¼Œåˆèµ·ä¾†ï¼ŒPOS å»¶çºŒå¾Œæ–¹è©å½™ã€‚å¦‚ï¼šæŠ—X/Yè» -> æŠ—XYè»
    ### æ‰¾åˆ°çµæ§‹
    verbPLIST = [x.group() for x in G_getVerbObjectStructurePhraseCountries_pat.finditer(joinSTR)]

    ### åˆæˆæ–°çµæ§‹
    newVerbPLIST = []
    for structure in verbPLIST:
        struLIST: list[str] = [x.group() for x in G_getText_pat.finditer(structure)]
        struSTR: str = "".join(struLIST)
        newVerbPLIST.append("<ENTY_intlRelation>" + struSTR + "</ENTY_intlRelation>")

    ### ä»£æ›¿åŸçµæ§‹
    for i in range(len(verbPLIST)):
        joinSTR = re.sub(verbPLIST[i], newVerbPLIST[i], joinSTR)

    # æ ¹æ“šæ–°çš„ result_pos ä¿®æ”¹ result_segmentation & result_obj
    resultPosLIST: list[str] = _splitResultPosSTR(joinSTR)
    resultDICT["result_pos"] = resultPosLIST
    resultDICT["result_obj"] = _getResultObjByResultPos(resultPosLIST)
    resultDICT["result_segmentation"] = _getResultSegmentationByResultPos(resultPosLIST)

    return resultDICT

def _mergeCountry(resultDICT: dict) -> dict:
    """
    å°‡å¤šå€‹åœ‹å®¶æ‹¼åœ¨ä¸€èµ·ï¼Œæ¨™è¨˜ POS ç‚º ENTY_countries
    å¦‚ï¼šæ—¥/ç¾æ¾³/å° -> æ—¥ç¾æ¾³å°ï¼›æ—¥ç¾æ¾³/å° -> æ—¥ç¾æ¾³å°
    """
    # éæ­·æª¢æŸ¥ result_pos
    newPosLIST: list[str] = _addSymbolToPunc(resultDICT["result_pos"])
    joinSTR: str = "".join(newPosLIST)

    ## æŠŠåœ‹å®¶åˆèµ·ä¾†ï¼Œå¦‚ï¼šç¾/æ—¥ä¸­ -> ç¾æ—¥ä¸­ï¼ŒPOS è¨­æˆ ENTY_countries
    ### æ‰¾åˆ°çµæ§‹
    countryLIST: list[str] = [x.group() for x in G_getCountry_pat.finditer(joinSTR)]

    if len(countryLIST)==0:
        return resultDICT

    ### åˆæˆæ–°çµæ§‹
    newCountryLIST: list[str] = []
    for structure in countryLIST:
        struLIST: list[str] = [x.group() for x in G_getText_pat.finditer(structure)]
        struSTR: str = "".join(struLIST)
        newCountryLIST.append("<ENTY_countries>" + struSTR + "</ENTY_countries>")

    ### ä»£æ›¿åŸçµæ§‹
    for i in range(len(countryLIST)):
        joinSTR = re.sub(countryLIST[i], newCountryLIST[i], joinSTR)

    ## é¡å¤–è™•ç†ç‰¹æ®Šåè©ï¼Œå¦‚ï¼šä¸­æ—¥/å¤§æˆ° -> ä¸­æ—¥å¤§æˆ°ï¼ŒPOS è¨­æˆ ENTY_intlRelation
    ### æ‰¾åˆ°çµæ§‹
    countryLIST = [x.group() for x in G_getCountryPhrase_pat.finditer(joinSTR)]

    ### åˆæˆæ–°çµæ§‹
    newCountryLIST = []
    for structure in countryLIST:
        struLIST: list[str] = [x.group() for x in G_getText_pat.finditer(structure)]
        struSTR: str = "".join(struLIST)
        newCountryLIST.append("<ENTY_intlRelation>" + struSTR + "</ENTY_intlRelation>")

     ### ä»£æ›¿åŸçµæ§‹
    for i in range(len(countryLIST)):
        joinSTR = re.sub(countryLIST[i], newCountryLIST[i], joinSTR)

    # æ ¹æ“šæ–°çš„ result_pos ä¿®æ”¹ result_segmentation & result_obj
    resultPosLIST: list[str] = _splitResultPosSTR(joinSTR)
    resultDICT["result_pos"] = resultPosLIST
    resultDICT["result_obj"] = _getResultObjByResultPos(resultPosLIST)
    resultDICT["result_segmentation"] = _getResultSegmentationByResultPos(resultPosLIST)

    return resultDICT

def _mergeCountryAbbr(resultDICT: dict) -> dict:
    """
    å°‡åœ‹å®¶å’Œåè©æ‹¼åœ¨ä¸€èµ·ï¼Œæ¨™è¨˜ POS ç‚º ENTY_countryAbbr
    å¦‚ï¼š[è‹±æ³•å¾·ä¿„æ³°éŸ“ç¾ä¸­...ç§˜å§”æ–å‰è«¾å][åª’è»ç±æ–¹åºœè£”è‚¡å•†è³‡ç”Ÿä¼åœ‹]
    """
    # éæ­·æª¢æŸ¥ result_pos
    newPosLIST: list[str] = _addSymbolToPunc(resultDICT["result_pos"])
    joinSTR: str = "".join(newPosLIST)

    ## æŠŠåœ‹å®¶åˆèµ·ä¾†ï¼Œå¦‚ï¼šç¾/æ—¥ä¸­ -> ç¾æ—¥ä¸­ï¼ŒPOS è¨­æˆ ENTY_countryAbbr
    ### æ‰¾åˆ°çµæ§‹
    countryAbbrLIST: list[str] = [x.group() for x in G_getCountryAbbr_pat.finditer(joinSTR)]

    if len(countryAbbrLIST)==0:
        return resultDICT

    ### åˆæˆæ–°çµæ§‹
    newCountryAbbrLIST = []
    for structure in countryAbbrLIST:
        struLIST: list[str] = [x.group() for x in G_getText_pat.finditer(structure)]
        struSTR: str = "".join(struLIST)
        newCountryAbbrLIST.append("<ENTY_countryAbbr>" + struSTR + "</ENTY_countryAbbr>")

    ### ä»£æ›¿åŸçµæ§‹
    for i in range(len(countryAbbrLIST)):
        joinSTR = re.sub(countryAbbrLIST[i], newCountryAbbrLIST[i], joinSTR)

    # æ ¹æ“šæ–°çš„ result_pos ä¿®æ”¹ result_segmentation & result_obj
    resultPosLIST: list[str] = _splitResultPosSTR(joinSTR)
    resultDICT["result_pos"] = resultPosLIST
    resultDICT["result_obj"] = _getResultObjByResultPos(resultPosLIST)
    resultDICT["result_segmentation"] = _getResultSegmentationByResultPos(resultPosLIST)

    return resultDICT

def _mergeMediaTerm(resultDICT: dict) -> dict:
    """
    åˆæˆæ–°èç”¨èªä¸­é—œæ–¼æ”¿æ²»çš„éƒ¨åˆ†ï¼Œæ¨™è¨˜ POS ç‚º ENTY_political
    å¦‚ï¼š[è—ç¶ ç™½][ç‡Ÿå§”]
    """
    # éæ­·æª¢æŸ¥ result_pos
    newPosLIST: list[str] = _addSymbolToPunc(resultDICT["result_pos"])
    joinSTR: str = "".join(newPosLIST)

    ## æŠŠæ–°èç”¨èªåˆèµ·ä¾†ï¼Œå¦‚ï¼šè—/ç‡Ÿ -> è—ç‡Ÿï¼ŒPOS è¨­æˆ ENTY_political
    ### æ‰¾åˆ°çµæ§‹
    mediaTermLIST: list[str] = [x.group() for x in G_getMediaTerm_pat.finditer(joinSTR)]

    if len(mediaTermLIST)==0:
        return resultDICT

    ### åˆæˆæ–°çµæ§‹
    newMediaTermLIST = []
    for structure in mediaTermLIST:
        struLIST: list[str] = [x.group() for x in G_getText_pat.finditer(structure)]
        struSTR: str = "".join(struLIST)
        newMediaTermLIST.append("<ENTY_political>" + struSTR + "</ENTY_political>")

    ### ä»£æ›¿åŸçµæ§‹
    for i in range(len(mediaTermLIST)):
        joinSTR = re.sub(mediaTermLIST[i], newMediaTermLIST[i], joinSTR)

    # æ ¹æ“šæ–°çš„ result_pos ä¿®æ”¹ result_segmentation & result_obj
    resultPosLIST: list[str] = _splitResultPosSTR(joinSTR)
    resultDICT["result_pos"] = resultPosLIST
    resultDICT["result_obj"] = _getResultObjByResultPos(resultPosLIST)
    resultDICT["result_segmentation"] = _getResultSegmentationByResultPos(resultPosLIST)

    return resultDICT

def _mergeUnit2Num(resultDICT: dict) -> dict:
    """
    å°‡è¢«åˆ‡é–‹çš„å–®ä½å’Œå‰æ–¹æ•¸å­—åˆæˆ
    e.g. ç¬¬22/å±† -> ç¬¬22å±†
    """
    # éæ­·æª¢æŸ¥ result_pos
    newPosLIST: list[str] = _addSymbolToPunc(resultDICT["result_pos"])
    joinSTR: str = "".join(newPosLIST)

    ## æŠŠå–®ä½å’Œæ•¸å­—åˆèµ·ä¾†ï¼Œå¦‚ï¼šç¬¬22/å±† -> ç¬¬22å±†ï¼ŒPOS è¨­æˆ ENTITY_DetPhrase
    ### æ‰¾åˆ°çµæ§‹
    unit2NumLIST: list[str] = [x.group() for x in G_getUnit2Num_pat.finditer(joinSTR)]

    if len(unit2NumLIST)==0:
        return resultDICT

    ### åˆæˆæ–°çµæ§‹
    newUnit2NumLIST = []
    for structure in unit2NumLIST:
        struLIST: list[str] = [x.group() for x in G_getText_pat.finditer(structure)]
        struSTR: str = "".join(struLIST)
        newUnit2NumLIST.append("<ENTITY_DetPhrase>" + struSTR + "</ENTITY_DetPhrase>")

    ### ä»£æ›¿åŸçµæ§‹
    for i in range(len(unit2NumLIST)):
        joinSTR = re.sub(unit2NumLIST[i], newUnit2NumLIST[i], joinSTR)

    # æ ¹æ“šæ–°çš„ result_pos ä¿®æ”¹ result_segmentation & result_obj
    resultPosLIST: list[str] = _splitResultPosSTR(joinSTR)
    resultDICT["result_pos"] = resultPosLIST
    resultDICT["result_obj"] = _getResultObjByResultPos(resultPosLIST)
    resultDICT["result_segmentation"] = _getResultSegmentationByResultPos(resultPosLIST)

    return resultDICT

def _mergeChemical2Modifier(resultDICT: dict) -> dict:
    """
    å°‡åŒ–å­¸å…ƒç´ å’Œå…¶å‰æ–¹çš„å½¢å®¹è©/å‰¯è©åˆä½µã€‚
    e.g. é«˜/éˆ£ -> é«˜éˆ£
    """
    # éæ­·æª¢æŸ¥ result_pos
    newPosLIST: list[str] = _addSymbolToPunc(resultDICT["result_pos"])
    joinSTR: str = "".join(newPosLIST)

    ## æŠŠåŒ–å­¸å…ƒç´ å’Œå…¶å‰æ–¹çš„å½¢å®¹è©/å‰¯è©åˆèµ·ä¾†ï¼Œå¦‚ï¼šé«˜/éˆ£ -> é«˜éˆ£ï¼ŒPOS è¨­æˆ ENTY_chemicalPattern
    ### æ‰¾åˆ°çµæ§‹
    chemical2ModifierLIST: list[str] = [x.group() for x in G_getChemical2Modifier_pat.finditer(joinSTR)]

    if len(chemical2ModifierLIST)==0:
        return resultDICT

    ### åˆæˆæ–°çµæ§‹
    newChemical2ModifierLIST = []
    for structure in chemical2ModifierLIST:
        struLIST: list[str] = [x.group() for x in G_getText_pat.finditer(structure)]
        struSTR: str = "".join(struLIST)
        newChemical2ModifierLIST.append("<ENTY_chemicalPattern>" + struSTR + "</ENTY_chemicalPattern>")

    ### ä»£æ›¿åŸçµæ§‹
    for i in range(len(chemical2ModifierLIST)):
        joinSTR = re.sub(chemical2ModifierLIST[i], newChemical2ModifierLIST[i], joinSTR)

    # æ ¹æ“šæ–°çš„ result_pos ä¿®æ”¹ result_segmentation & result_obj
    resultPosLIST: list[str] = _splitResultPosSTR(joinSTR)
    resultDICT["result_pos"] = resultPosLIST
    resultDICT["result_obj"] = _getResultObjByResultPos(resultPosLIST)
    resultDICT["result_segmentation"] = _getResultSegmentationByResultPos(resultPosLIST)

    return resultDICT

def _mergeJobTitle(resultDICT: dict) -> dict:
    """
    åˆæˆå·¥ä½œå–®ä½å’Œè·ç¨±ï¼Œæ¨™è¨˜ POS ç‚º KNOWLEDGE_jobTitle
    å¦‚ï¼šæ•™è‚²å±€/é•· -> æ•™è‚²å±€é•·
    """
    # éæ­·æª¢æŸ¥ result_pos
    newPosLIST: list[str] = _addSymbolToPunc(resultDICT["result_pos"])
    joinSTR: str = "".join(newPosLIST)

    ## æŠŠå·¥ä½œå–®ä½å’Œè·ç¨±åˆèµ·ä¾†ï¼Œå¦‚ï¼šæ•™è‚²å±€/é•· -> æ•™è‚²å±€é•·ï¼ŒPOS è¨­ç‚º KNOWLEDGE_jobTitle
    ### æ‰¾åˆ°çµæ§‹
    jobTitleLIST: list[str] = [x.group() for x in G_getJobTitle_pat.finditer(joinSTR)]

    if len(jobTitleLIST)!=0:
        ### åˆæˆæ–°çµæ§‹
        newJobTitleLIST: list[str] = []
        for structure in jobTitleLIST:
            struLIST: list[str] = [x.group() for x in G_getText_pat.finditer(structure)]
            struSTR: str = "".join(struLIST)
            newJobTitleLIST.append("<KNOWLEDGE_jobTitle>" + struSTR + "</KNOWLEDGE_jobTitle>")

        ### ä»£æ›¿åŸçµæ§‹
        for i in range(len(jobTitleLIST)):
            joinSTR = re.sub(jobTitleLIST[i], newJobTitleLIST[i], joinSTR)

    ## è‹¥åŸæœ¬æ–·è©çµæœå°±æ˜¯ xxå±€é•· ï¼Œå°‡å…¶ POS ä¸€å¾‹æ”¹ç‚º KNOWLEDGE_jobTitle
    ### æ‰¾åˆ°çµæ§‹
    jobTitleLIST = [x.group() for x in G_getJobTitlePhrase_pat.finditer(joinSTR)]

    if len(jobTitleLIST) != 0:
        ### åˆæˆæ–°çµæ§‹
        newJobTitleLIST: list[str] = []
        for structure in jobTitleLIST:
            struLIST: list[str] = [x.group() for x in G_getText_pat.finditer(structure)]
            struSTR: str = "".join(struLIST)
            newJobTitleLIST.append("<KNOWLEDGE_jobTitle>" + struSTR + "</KNOWLEDGE_jobTitle>")

        ### ä»£æ›¿åŸçµæ§‹
        for i in range(len(jobTitleLIST)):
            joinSTR = re.sub(jobTitleLIST[i], newJobTitleLIST[i], joinSTR)

    # æ ¹æ“šæ–°çš„ result_pos ä¿®æ”¹ result_segmentation & result_obj
    resultPosLIST: list[str] = _splitResultPosSTR(joinSTR)
    resultDICT["result_pos"] = resultPosLIST
    resultDICT["result_obj"] = _getResultObjByResultPos(resultPosLIST)
    resultDICT["result_segmentation"] = _getResultSegmentationByResultPos(resultPosLIST)

    return resultDICT

def _splitResultPosSTR(joinSTR: str) -> list[str]:
    joinSTR = joinSTR.replace("â†­â†­", "â†­")
    return joinSTR.split("â†­")

def _addSymbolToPunc(resultPosLIST: list[str]) -> list[str]:
    """
    å°‡æ¨™é»ç¬¦è™Ÿçš„å‰å¾Œéƒ½æ–°å¢ä¸€å€‹ â†­ ï¼Œä»¥ä¾¿å¾ŒçºŒåˆ‡é–‹ã€‚
    """
    newResultPosLIST: list[str] = []

    for i in range(len(resultPosLIST)):
        newSentenceSTR: str = ""
        if len(resultPosLIST[i]) == 1:
            if i==0:
                newSentenceSTR = resultPosLIST[i] + "â†­"
            elif i==len(resultPosLIST)-1:
                newSentenceSTR = "â†­" + resultPosLIST[i]
            else:
                newSentenceSTR = "â†­"+ resultPosLIST[i] + "â†­"

        else:
            newSentenceSTR = resultPosLIST[i]

        newResultPosLIST.append(newSentenceSTR)

    return newResultPosLIST